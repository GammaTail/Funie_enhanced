{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d3a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from ntpath import basename\n",
    "from os.path import join, exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a4a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    data_dir = \"data/A/1.jpg\"\n",
    "    sample_dir = \"data/output/\"\n",
    "    model_name = \"funiegan\"  # or \"ugan\"\n",
    "    model_path = \"checkpoints/FunieGAN/paired/generator_60.pth\"\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6587482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "assert exists(opt.model_path), \"model not found\"\n",
    "os.makedirs(opt.sample_dir, exist_ok=True)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "Tensor = torch.cuda.FloatTensor if is_cuda else torch.FloatTensor \n",
    "print(is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762cd0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.model_name.lower()=='funiegan':\n",
    "    from nets import funiegan\n",
    "    model = funiegan.GeneratorFunieGAN()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e2e64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoints/FunieGAN/paired/generator_60.pth\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(opt.model_path))\n",
    "if is_cuda: model.cuda()\n",
    "model.eval()\n",
    "print (\"Loaded model from %s\" % (opt.model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801a8b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height, channels = 1024, 1024, 3\n",
    "transforms_ = [transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "               transforms.ToTensor(),\n",
    "               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]\n",
    "transform = transforms.Compose(transforms_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f97524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: funiegan\n",
      "Input folder: data/A/1.jpg\n",
      "Output folder: data/output/\n",
      "Looking for test images...\n",
      "Found 0 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model loaded: {opt.model_name}\")\n",
    "print(f\"Input folder: {opt.data_dir}\")\n",
    "print(f\"Output folder: {opt.sample_dir}\")\n",
    "print(f\"Looking for test images...\")\n",
    "\n",
    "test_files = sorted(glob(join(opt.data_dir, \"*\")))\n",
    "print(f\"Found {len(test_files)} images.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "144a2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458f7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/A/img.jpg\n",
      "Inference time: 0.296 sec\n",
      "Saved output to: data/output/img.jpg\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import time\n",
    "from os.path import join, basename\n",
    "\n",
    "img_width, img_height = 256, 256  # Match training size\n",
    "transforms_ = [\n",
    "    transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "transform = transforms.Compose(transforms_)\n",
    "\n",
    "input_image_path = \"data/A/img.jpg\"\n",
    "\n",
    "try:\n",
    "    print(f\"Processing {input_image_path}\")\n",
    "    inp_img = transform(Image.open(input_image_path).convert(\"RGB\"))\n",
    "    inp_img = Variable(inp_img).unsqueeze(0).type(torch.FloatTensor).to(device)  # device = 'cuda' or 'cpu'\n",
    "\n",
    "    s = time.time()\n",
    "    with torch.no_grad():\n",
    "        gen_img = model(inp_img)\n",
    "    elapsed = time.time() - s\n",
    "    print(f\"Inference time: {elapsed:.3f} sec\")\n",
    "\n",
    "    # Save side-by-side input/output\n",
    "    img_sample = torch.cat((inp_img.data, gen_img.data), -1)\n",
    "    save_path = join(opt.sample_dir, basename(input_image_path))\n",
    "    save_image(img_sample, save_path, normalize=True)\n",
    "    print(f\"Saved output to: {save_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing image: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49b4f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.004 sec\n",
      "Saved FUNIE-GAN output to: output\\funie_output.jpg\n",
      "✅ Corrected image saved at: output\\funie_corrected.jpg\n",
      "Saved comparison image.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from os.path import join, basename\n",
    "\n",
    "# ----- Config -----\n",
    "input_image_path = \"data/A/1.jpg\"  # Your test image\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_width, img_height = 256, 256\n",
    "\n",
    "# ----- Preprocessing -----\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# ----- Load and Run FUNIE-GAN -----\n",
    "inp_img = transform(Image.open(input_image_path).convert(\"RGB\"))\n",
    "inp_img = Variable(inp_img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = time.time()\n",
    "    gen_img = model(inp_img)\n",
    "    print(f\"Inference time: {time.time() - start:.3f} sec\")\n",
    "\n",
    "# Save raw output\n",
    "gen_path = join(output_dir, \"funie_output.jpg\")\n",
    "save_image(gen_img.data, gen_path, normalize=True)\n",
    "print(f\"Saved FUNIE-GAN output to: {gen_path}\")\n",
    "\n",
    "# ----- Post-processing -----\n",
    "def white_balance(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    avg_a, avg_b = np.mean(a), np.mean(b)\n",
    "    \n",
    "    a = a.astype(np.float32)\n",
    "    b = b.astype(np.float32)\n",
    "    l_scaled = l.astype(np.float32) / 255.0\n",
    "\n",
    "    a = a - ((avg_a - 128) * l_scaled * 1.1)\n",
    "    b = b - ((avg_b - 128) * l_scaled * 1.1)\n",
    "\n",
    "    a = np.clip(a, 0, 255).astype(np.uint8)\n",
    "    b = np.clip(b, 0, 255).astype(np.uint8)\n",
    "\n",
    "    lab = cv2.merge([l, a, b])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def clahe_contrast(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def gamma_correction(img, gamma=1.2):\n",
    "    inv_gamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "# Load GAN output as BGR\n",
    "gan_img = cv2.imread(gen_path)\n",
    "wb_img = white_balance(gan_img)\n",
    "clahe_img = clahe_contrast(wb_img)\n",
    "final_img = gamma_correction(clahe_img, gamma=1.1)\n",
    "\n",
    "# Save final enhanced result\n",
    "final_path = join(output_dir, \"funie_corrected.jpg\")\n",
    "final_img = np.clip(final_img, 0, 255).astype(np.uint8)\n",
    "success = cv2.imwrite(final_path, final_img)\n",
    "if success:\n",
    "    print(f\"✅ Corrected image saved at: {final_path}\")\n",
    "else:\n",
    "    print(f\"❌ Failed to save image at: {final_path}\")\n",
    "\n",
    "# ----- Side-by-side visual -----\n",
    "orig = cv2.resize(cv2.imread(input_image_path), (256, 256))\n",
    "gan = cv2.resize(gan_img, (256, 256))\n",
    "post = cv2.resize(final_img, (256, 256))\n",
    "comparison = cv2.hconcat([orig, gan, post])\n",
    "cv2.imwrite(join(output_dir, \"comparison.jpg\"), comparison)\n",
    "print(\"Saved comparison image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: data/A/vid.mp4\n",
      "Processed frame 1\n",
      "Processed frame 2\n",
      "Processed frame 3\n",
      "Processed frame 4\n",
      "Processed frame 5\n",
      "Processed frame 6\n",
      "Processed frame 7\n",
      "Processed frame 8\n",
      "Processed frame 9\n",
      "Processed frame 10\n",
      "Processed frame 11\n",
      "Processed frame 12\n",
      "Processed frame 13\n",
      "Processed frame 14\n",
      "Processed frame 15\n",
      "Processed frame 16\n",
      "Processed frame 17\n",
      "Processed frame 18\n",
      "Processed frame 19\n",
      "Processed frame 20\n",
      "Processed frame 21\n",
      "Processed frame 22\n",
      "Processed frame 23\n",
      "Processed frame 24\n",
      "Processed frame 25\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "input_video_path = \"data/A/vidd.mp4\"  # <-- your video input\n",
    "output_video_path = join(opt.sample_dir, \"enhanced_\" + basename(input_video_path))\n",
    "\n",
    "try:\n",
    "    print(f\"Processing video: {input_video_path}\")\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {input_video_path}\")\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Output writer: double width for side-by-side display\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "    times = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize + convert to PIL for model input\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((256, 256), Image.BICUBIC)\n",
    "        inp_tensor = transform(pil_img).unsqueeze(0).type(Tensor)\n",
    "\n",
    "        # Inference\n",
    "        s = time.time()\n",
    "        with torch.no_grad():\n",
    "            gen_tensor = model(inp_tensor)\n",
    "        times.append(time.time() - s)\n",
    "\n",
    "        # Convert output to numpy\n",
    "        inp_np = inp_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "        gen_np = gen_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "        # Denormalize\n",
    "        inp_np = ((inp_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "        gen_np = ((gen_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "        # Resize output back to original video size\n",
    "        inp_resized = cv2.resize(inp_np, (width, height))\n",
    "        gen_resized = cv2.resize(gen_np, (width, height))\n",
    "\n",
    "        # Stack side-by-side and convert to BGR for OpenCV\n",
    "        side_by_side = cv2.hconcat([\n",
    "            cv2.cvtColor(inp_resized, cv2.COLOR_RGB2BGR),\n",
    "            cv2.cvtColor(gen_resized, cv2.COLOR_RGB2BGR)\n",
    "        ])\n",
    "\n",
    "        out.write(side_by_side)\n",
    "        frame_count += 1\n",
    "        print(f\"Processed frame {frame_count}\")\n",
    "\n",
    "    # Release everything\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    if len(times) > 1:\n",
    "        Ttime, Mtime = np.sum(times[1:]), np.mean(times[1:])\n",
    "        print(f\"\\nProcessed {frame_count} frames.\")\n",
    "        print(f\"Time taken: {Ttime:.2f} sec at {1. / Mtime:.2f} fps\")\n",
    "        print(f\"Saved enhanced video to: {output_video_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing video: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab14118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: data/A/vidd2.mp4\n",
      "Processed frame 1/128\n",
      "Processed frame 2/128\n",
      "Processed frame 3/128\n",
      "Processed frame 4/128\n",
      "Processed frame 5/128\n",
      "Processed frame 6/128\n",
      "Processed frame 7/128\n",
      "Processed frame 8/128\n",
      "Processed frame 9/128\n",
      "Processed frame 10/128\n",
      "Processed frame 11/128\n",
      "Processed frame 12/128\n",
      "Processed frame 13/128\n",
      "Processed frame 14/128\n",
      "Processed frame 15/128\n",
      "Processed frame 16/128\n",
      "Processed frame 17/128\n",
      "Processed frame 18/128\n",
      "Processed frame 19/128\n",
      "Processed frame 20/128\n",
      "Processed frame 21/128\n",
      "Processed frame 22/128\n",
      "Processed frame 23/128\n",
      "Processed frame 24/128\n",
      "Processed frame 25/128\n",
      "Processed frame 26/128\n",
      "Processed frame 27/128\n",
      "Processed frame 28/128\n",
      "Processed frame 29/128\n",
      "Processed frame 30/128\n",
      "Processed frame 31/128\n",
      "Processed frame 32/128\n",
      "Processed frame 33/128\n",
      "Processed frame 34/128\n",
      "Processed frame 35/128\n",
      "Processed frame 36/128\n",
      "Processed frame 37/128\n",
      "Processed frame 38/128\n",
      "Processed frame 39/128\n",
      "Processed frame 40/128\n",
      "Processed frame 41/128\n",
      "Processed frame 42/128\n",
      "Processed frame 43/128\n",
      "Processed frame 44/128\n",
      "Processed frame 45/128\n",
      "Processed frame 46/128\n",
      "Processed frame 47/128\n",
      "Processed frame 48/128\n",
      "Processed frame 49/128\n",
      "Processed frame 50/128\n",
      "Processed frame 51/128\n",
      "Processed frame 52/128\n",
      "Processed frame 53/128\n",
      "Processed frame 54/128\n",
      "Processed frame 55/128\n",
      "Processed frame 56/128\n",
      "Processed frame 57/128\n",
      "Processed frame 58/128\n",
      "Processed frame 59/128\n",
      "Processed frame 60/128\n",
      "Processed frame 61/128\n",
      "Processed frame 62/128\n",
      "Processed frame 63/128\n",
      "Processed frame 64/128\n",
      "Processed frame 65/128\n",
      "Processed frame 66/128\n",
      "Processed frame 67/128\n",
      "Processed frame 68/128\n",
      "Processed frame 69/128\n",
      "Processed frame 70/128\n",
      "Processed frame 71/128\n",
      "Processed frame 72/128\n",
      "Processed frame 73/128\n",
      "Processed frame 74/128\n",
      "Processed frame 75/128\n",
      "Processed frame 76/128\n",
      "Processed frame 77/128\n",
      "Processed frame 78/128\n",
      "Processed frame 79/128\n",
      "Processed frame 80/128\n",
      "Processed frame 81/128\n",
      "Processed frame 82/128\n",
      "Processed frame 83/128\n",
      "Processed frame 84/128\n",
      "Processed frame 85/128\n",
      "Processed frame 86/128\n",
      "Processed frame 87/128\n",
      "Processed frame 88/128\n",
      "Processed frame 89/128\n",
      "Processed frame 90/128\n",
      "Processed frame 91/128\n",
      "Processed frame 92/128\n",
      "Processed frame 93/128\n",
      "Processed frame 94/128\n",
      "Processed frame 95/128\n",
      "Processed frame 96/128\n",
      "Processed frame 97/128\n",
      "Processed frame 98/128\n",
      "Processed frame 99/128\n",
      "Processed frame 100/128\n",
      "Processed frame 101/128\n",
      "Processed frame 102/128\n",
      "Processed frame 103/128\n",
      "Processed frame 104/128\n",
      "Processed frame 105/128\n",
      "Processed frame 106/128\n",
      "Processed frame 107/128\n",
      "Processed frame 108/128\n",
      "Processed frame 109/128\n",
      "Processed frame 110/128\n",
      "Processed frame 111/128\n",
      "Processed frame 112/128\n",
      "Processed frame 113/128\n",
      "Processed frame 114/128\n",
      "Processed frame 115/128\n",
      "Processed frame 116/128\n",
      "Processed frame 117/128\n",
      "Processed frame 118/128\n",
      "Processed frame 119/128\n",
      "Processed frame 120/128\n",
      "Processed frame 121/128\n",
      "Processed frame 122/128\n",
      "Processed frame 123/128\n",
      "Processed frame 124/128\n",
      "Processed frame 125/128\n",
      "Processed frame 126/128\n",
      "Processed frame 127/128\n",
      "Processed frame 128/128\n",
      "\n",
      "Processed 128 frames.\n",
      "Time taken: 0.77 sec at 167.29 fps\n",
      "Saved enhanced video to: data/output/enhancedd_vidd2.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from os.path import join, basename\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "# === Paths ===\n",
    "input_video_path = \"data/A/vidd2.mp4\"\n",
    "output_video_path = join(opt.sample_dir, \"enhancedd_\" + basename(input_video_path))\n",
    "\n",
    "# === Tensor type ===\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# === Transform (same as model training) ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Adjust based on training size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# === Model: Ensure it is loaded ===\n",
    "# model = YourModelClass()\n",
    "# model.load_state_dict(torch.load(\"your_model_path.pth\"))\n",
    "# model.eval()\n",
    "# model.to(\"cuda\" or \"cpu\")\n",
    "\n",
    "try:\n",
    "    print(f\"Processing video: {input_video_path}\")\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {input_video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    max_frames = int(fps * 4)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "    times = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert frame to tensor\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        inp_tensor = transform(pil_img).unsqueeze(0).type(Tensor)\n",
    "        inp_tensor = Variable(inp_tensor)\n",
    "\n",
    "        # === Model Inference ===\n",
    "        s = time.time()\n",
    "        with torch.no_grad():\n",
    "            gen_tensor = model(inp_tensor)\n",
    "        times.append(time.time() - s)\n",
    "\n",
    "        # === Postprocessing ===\n",
    "        gen_np = gen_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "        gen_np = ((gen_np * 0.5 + 0.5) * 255).astype(np.uint8)  # De-normalize\n",
    "\n",
    "        # === Apply Gaussian Blur ===\n",
    "        gen_np = cv2.GaussianBlur(gen_np, (5, 5), sigmaX=1.0)\n",
    "\n",
    "        # === Apply Sharpening Filter ===\n",
    "        sharpen_kernel = np.array([[0, -1, 0],\n",
    "                                   [-1, 5, -1],\n",
    "                                   [0, -1, 0]])\n",
    "        gen_np = cv2.filter2D(gen_np, -1, sharpen_kernel)\n",
    "\n",
    "        # === Resize for final video output ===\n",
    "        inp_np = inp_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "        inp_np = ((inp_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "        inp_np = cv2.resize(inp_np, (width, height))\n",
    "        gen_np = cv2.resize(gen_np, (width, height))\n",
    "\n",
    "        # === Concatenate side-by-side ===\n",
    "        side_by_side = cv2.hconcat([\n",
    "            cv2.cvtColor(inp_np, cv2.COLOR_RGB2BGR),\n",
    "            cv2.cvtColor(gen_np, cv2.COLOR_RGB2BGR)\n",
    "        ])\n",
    "        out.write(side_by_side)\n",
    "        frame_count += 1\n",
    "        print(f\"Processed frame {frame_count}/{max_frames}\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    if len(times) > 0:\n",
    "        Ttime, Mtime = np.sum(times), np.mean(times)\n",
    "        print(f\"\\nProcessed {frame_count} frames.\")\n",
    "        print(f\"Time taken: {Ttime:.2f} sec at {1. / Mtime:.2f} fps\")\n",
    "        print(f\"Saved enhanced video to: {output_video_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing video: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b74d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: data/A/vidd2.mp4\n",
      "Processed frame 1/128\n",
      "Processed frame 2/128\n",
      "Processed frame 3/128\n",
      "Processed frame 4/128\n",
      "Processed frame 5/128\n",
      "Processed frame 6/128\n",
      "Processed frame 7/128\n",
      "Processed frame 8/128\n",
      "Processed frame 9/128\n",
      "Processed frame 10/128\n",
      "Processed frame 11/128\n",
      "Processed frame 12/128\n",
      "Processed frame 13/128\n",
      "Processed frame 14/128\n",
      "Processed frame 15/128\n",
      "Processed frame 16/128\n",
      "Processed frame 17/128\n",
      "Processed frame 18/128\n",
      "Processed frame 19/128\n",
      "Processed frame 20/128\n",
      "Processed frame 21/128\n",
      "Processed frame 22/128\n",
      "Processed frame 23/128\n",
      "Processed frame 24/128\n",
      "Processed frame 25/128\n",
      "Processed frame 26/128\n",
      "Processed frame 27/128\n",
      "Processed frame 28/128\n",
      "Processed frame 29/128\n",
      "Processed frame 30/128\n",
      "Processed frame 31/128\n",
      "Processed frame 32/128\n",
      "Processed frame 33/128\n",
      "Processed frame 34/128\n",
      "Processed frame 35/128\n",
      "Processed frame 36/128\n",
      "Processed frame 37/128\n",
      "Processed frame 38/128\n",
      "Processed frame 39/128\n",
      "Processed frame 40/128\n",
      "Processed frame 41/128\n",
      "Processed frame 42/128\n",
      "Processed frame 43/128\n",
      "Processed frame 44/128\n",
      "Processed frame 45/128\n",
      "Processed frame 46/128\n",
      "Processed frame 47/128\n",
      "Processed frame 48/128\n",
      "Processed frame 49/128\n",
      "Processed frame 50/128\n",
      "Processed frame 51/128\n",
      "Processed frame 52/128\n",
      "Processed frame 53/128\n",
      "Processed frame 54/128\n",
      "Processed frame 55/128\n",
      "Processed frame 56/128\n",
      "Processed frame 57/128\n",
      "Processed frame 58/128\n",
      "Processed frame 59/128\n",
      "Processed frame 60/128\n",
      "Processed frame 61/128\n",
      "Processed frame 62/128\n",
      "Processed frame 63/128\n",
      "Processed frame 64/128\n",
      "Processed frame 65/128\n",
      "Processed frame 66/128\n",
      "Processed frame 67/128\n",
      "Processed frame 68/128\n",
      "Processed frame 69/128\n",
      "Processed frame 70/128\n",
      "Processed frame 71/128\n",
      "Processed frame 72/128\n",
      "Processed frame 73/128\n",
      "Processed frame 74/128\n",
      "Processed frame 75/128\n",
      "Processed frame 76/128\n",
      "Processed frame 77/128\n",
      "Processed frame 78/128\n",
      "Processed frame 79/128\n",
      "Processed frame 80/128\n",
      "Processed frame 81/128\n",
      "Processed frame 82/128\n",
      "Processed frame 83/128\n",
      "Processed frame 84/128\n",
      "Processed frame 85/128\n",
      "Processed frame 86/128\n",
      "Processed frame 87/128\n",
      "Processed frame 88/128\n",
      "Processed frame 89/128\n",
      "Processed frame 90/128\n",
      "Processed frame 91/128\n",
      "Processed frame 92/128\n",
      "Processed frame 93/128\n",
      "Processed frame 94/128\n",
      "Processed frame 95/128\n",
      "Processed frame 96/128\n",
      "Processed frame 97/128\n",
      "Processed frame 98/128\n",
      "Processed frame 99/128\n",
      "Processed frame 100/128\n",
      "Processed frame 101/128\n",
      "Processed frame 102/128\n",
      "Processed frame 103/128\n",
      "Processed frame 104/128\n",
      "Processed frame 105/128\n",
      "Processed frame 106/128\n",
      "Processed frame 107/128\n",
      "Processed frame 108/128\n",
      "Processed frame 109/128\n",
      "Processed frame 110/128\n",
      "Processed frame 111/128\n",
      "Processed frame 112/128\n",
      "Processed frame 113/128\n",
      "Processed frame 114/128\n",
      "Processed frame 115/128\n",
      "Processed frame 116/128\n",
      "Processed frame 117/128\n",
      "Processed frame 118/128\n",
      "Processed frame 119/128\n",
      "Processed frame 120/128\n",
      "Processed frame 121/128\n",
      "Processed frame 122/128\n",
      "Processed frame 123/128\n",
      "Processed frame 124/128\n",
      "Processed frame 125/128\n",
      "Processed frame 126/128\n",
      "Processed frame 127/128\n",
      "Processed frame 128/128\n",
      "\n",
      "Processed 128 frames.\n",
      "Saved enhanced video to: data/output/enhancedd_vidd2.mp4\n",
      "Model inference time for last frame: 0.0211 sec (47.37 fps)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from os.path import join, basename\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "# === POST-PROCESSING FUNCTIONS ===\n",
    "\n",
    "def white_balance_safe(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    avg_a = np.mean(a)\n",
    "    avg_b = np.mean(b)\n",
    "    l_scaled = l.astype(np.float32) / 255.0\n",
    "    a = np.clip(a.astype(np.float32) - ((avg_a - 128) * l_scaled * 0.6), 0, 255)\n",
    "    b = np.clip(b.astype(np.float32) - ((avg_b - 128) * l_scaled * 0.6), 0, 255)\n",
    "    lab = cv2.merge([l, a.astype(np.uint8), b.astype(np.uint8)])\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def clahe_contrast_safe(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    lab = cv2.merge((cl, a, b))\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "def gamma_correction_safe(img, gamma=1.05):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([(i / 255.0) ** invGamma * 255 for i in range(256)]).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "def denoise_if_needed(img):\n",
    "    return cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 21)\n",
    "\n",
    "\n",
    "# === MAIN VIDEO ENHANCEMENT CODE ===\n",
    "\n",
    "# Inputs\n",
    "input_video_path = \"data/A/vidd2.mp4\"\n",
    "output_video_path = join(opt.sample_dir, \"enhancedd_\" + basename(input_video_path))\n",
    "\n",
    "# Tensor type\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "try:\n",
    "    print(f\"Processing video: {input_video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video: {input_video_path}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    max_frames = int(fps * 4)  # limit to ~50 seconds\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    single_frame_time = None\n",
    "\n",
    "    while frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert to PIL\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        inp_tensor = transform(pil_img).unsqueeze(0)\n",
    "        inp_tensor = Variable(inp_tensor).type(Tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if frame_count == max_frames - 1:\n",
    "                start = time.time()\n",
    "            gen_tensor = model(inp_tensor)\n",
    "            if frame_count == max_frames - 1:\n",
    "                torch.cuda.synchronize()\n",
    "                single_frame_time = time.time() - start\n",
    "\n",
    "        # Post-process the tensor\n",
    "        gen_tensor = torch.nan_to_num(gen_tensor, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "        inp_np = inp_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "        gen_np = gen_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "        inp_np = ((inp_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "        gen_np = ((gen_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "        inp_resized = cv2.resize(inp_np, (width, height))\n",
    "        gen_resized = cv2.resize(gen_np, (width, height))\n",
    "\n",
    "        # BGR conversion\n",
    "        gen_bgr = cv2.cvtColor(gen_resized, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Safe post-processing\n",
    "        denoised = denoise_if_needed(gen_bgr)\n",
    "        wb_img = white_balance_safe(denoised)\n",
    "        clahe_img = clahe_contrast_safe(wb_img)\n",
    "        final_img = gamma_correction_safe(clahe_img, gamma=1.05)\n",
    "\n",
    "        side_by_side = cv2.hconcat([\n",
    "            cv2.cvtColor(inp_resized, cv2.COLOR_RGB2BGR),\n",
    "            final_img\n",
    "        ])\n",
    "        out.write(side_by_side)\n",
    "\n",
    "        frame_count += 1\n",
    "        print(f\"Processed frame {frame_count}/{max_frames}\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    print(f\"\\nProcessed {frame_count} frames.\")\n",
    "    print(f\"Saved enhanced video to: {output_video_path}\")\n",
    "    if single_frame_time:\n",
    "        print(f\"Model inference time for last frame: {single_frame_time:.4f} sec ({1. / single_frame_time:.2f} fps)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing video: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c30eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Model preparation\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Transform (adjust to match training settings)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  # or your model's input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame.\")\n",
    "        break\n",
    "\n",
    "    # Preprocess frame\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    inp_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Inference timing\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        gen_tensor = model(inp_tensor)\n",
    "    torch.cuda.synchronize()\n",
    "    infer_time = time.time() - start\n",
    "\n",
    "    # Postprocess output\n",
    "    inp_np = inp_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "    gen_np = gen_tensor.squeeze().cpu().numpy().transpose(1, 2, 0)\n",
    "    inp_np = ((inp_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "    gen_np = ((gen_np * 0.5 + 0.5) * 255).astype(np.uint8)\n",
    "\n",
    "    # Resize to webcam resolution\n",
    "    height, width = frame.shape[:2]\n",
    "    inp_resized = cv2.resize(inp_np, (width, height))\n",
    "    gen_resized = cv2.resize(gen_np, (width, height))\n",
    "\n",
    "    # Combine and show\n",
    "    combined = cv2.hconcat([\n",
    "        cv2.cvtColor(inp_resized, cv2.COLOR_RGB2BGR),\n",
    "        cv2.cvtColor(gen_resized, cv2.COLOR_RGB2BGR)\n",
    "    ])\n",
    "    fps = 1. / infer_time\n",
    "    cv2.putText(combined, f\"{infer_time:.3f}s ({fps:.2f} FPS)\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Original | Enhanced\", combined)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a81bda61",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "test_files = sorted(glob(join(opt.data_dir, \"*.*\")))\n",
    "for path in test_files:\n",
    "    try:\n",
    "        print(f\"Processing {path}\")\n",
    "        inp_img = transform(Image.open(path).convert(\"RGB\"))\n",
    "        inp_img = Variable(inp_img).type(Tensor).unsqueeze(0)\n",
    "        s = time.time()\n",
    "        gen_img = model(inp_img)\n",
    "        times.append(time.time()-s)\n",
    "        img_sample = torch.cat((inp_img.data, gen_img.data), -1)\n",
    "        save_image(img_sample, join(opt.sample_dir, basename(path)), normalize=True)\n",
    "        print (\"Tested: %s\" % path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "\n",
    "## run-time    \n",
    "if (len(times) > 1):\n",
    "    print (\"\\nTotal samples: %d\" % len(test_files)) \n",
    "    # accumulate frame processing times (without bootstrap)\n",
    "    Ttime, Mtime = np.sum(times[1:]), np.mean(times[1:]) \n",
    "    print (\"Time taken: %d sec at %0.3f fps\" %(Ttime, 1./Mtime))\n",
    "    print(\"Saved generated images in in %s\\n\" %(opt.sample_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix2pix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
