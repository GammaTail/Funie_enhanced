{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156b24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# pytorch libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "# local libs\n",
    "from nets.commons import Weights_Normal, VGG19_PercepLoss\n",
    "from nets.funiegan import GeneratorFunieGAN, DiscriminatorFunieGAN\n",
    "from utils.data_utils import GetTrainingPairs, GetValImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd147793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    cfg_file = \"configs/train_euvp.yaml\"\n",
    "    epoch = 1\n",
    "    num_epochs = 60\n",
    "    batch_size = 8\n",
    "    lr = 0.0003\n",
    "    b1 = 0.5\n",
    "    b2 = 0.99\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69390e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "epoch = args.epoch\n",
    "num_epochs = args.num_epochs\n",
    "batch_size =  args.batch_size\n",
    "lr_rate, lr_b1, lr_b2 = args.lr, args.b1, args.b2 \n",
    "# load the data config file\n",
    "with open(args.cfg_file) as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "# get info from config file\n",
    "dataset_name = cfg[\"dataset_name\"] \n",
    "dataset_path = cfg[\"dataset_path\"]\n",
    "channels = cfg[\"chans\"]\n",
    "img_width = cfg[\"im_width\"]\n",
    "img_height = cfg[\"im_height\"] \n",
    "val_interval = cfg[\"val_interval\"]\n",
    "ckpt_interval = cfg[\"ckpt_interval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e554d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "samples_dir = os.path.join(\"samples/FunieGAN/\", dataset_name)\n",
    "checkpoint_dir = os.path.join(\"checkpoints/FunieGAN/\", dataset_name)\n",
    "os.makedirs(samples_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pubgp\\anaconda3\\envs\\pix2pix\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pubgp\\anaconda3\\envs\\pix2pix\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "Adv_cGAN = torch.nn.MSELoss()\n",
    "L1_G  = torch.nn.L1Loss() # similarity loss (l1)\n",
    "L_vgg = VGG19_PercepLoss() # content loss (vgg)\n",
    "lambda_1, lambda_con = 6, 2 # 7:3 (as in paper)\n",
    "patch = (1, img_height//16, img_width//16) # 16x16 for 256x256\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = GeneratorFunieGAN()\n",
    "discriminator = DiscriminatorFunieGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7fdfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    Adv_cGAN.cuda()\n",
    "    L1_G = L1_G.cuda()\n",
    "    L_vgg = L_vgg.cuda()\n",
    "    Tensor = torch.cuda.FloatTensor\n",
    "else:\n",
    "    Tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543517f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.epoch == 0:\n",
    "#     generator.apply(Weights_Normal)\n",
    "#     discriminator.apply(Weights_Normal)\n",
    "# else:\n",
    "#     generator.load_state_dict(torch.load(\"checkpoints/FunieGAN/%s/generator_%d.pth\" % (dataset_name, args.epoch)))\n",
    "#     discriminator.load_state_dict(torch.load(\"checkpoints/FunieGAN/%s/discriminator_%d.pth\" % (dataset_name, epoch)))\n",
    "#     print (\"Loaded model from epoch %d\" %(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14a7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_rate, betas=(lr_b1, lr_b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8921137b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: output\n",
      "Dataset Name: paired\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Path:\", dataset_path)\n",
    "print(\"Dataset Name:\", dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3efa762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FilesA: 905\n",
      "FilesB: 905\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "filesA = sorted(glob.glob(\"output/paired/input/*.*\"))\n",
    "filesB = sorted(glob.glob(\"output/paired/target/*.*\"))\n",
    "print(\"FilesA:\", len(filesA))\n",
    "print(\"FilesB:\", len(filesB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f462e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 905 input images and 905 target images.\n",
      "Loaded 905 input images.\n"
     ]
    }
   ],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataset_path = \"output/\"\n",
    "dataset_name = \"paired\"\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    GetTrainingPairs(dataset_path, dataset_name, transforms_=transforms_),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    GetValImage(dataset_path, dataset_name, transforms_=transforms_),\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92c0c70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 905 input images.\n",
      "Length from self.len: 905\n"
     ]
    }
   ],
   "source": [
    "dataset = GetValImage(dataset_path, dataset_name, transforms_=transforms_)\n",
    "print(\"Length from self.len:\", dataset.len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec96bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "\n",
    "# start_time = timeit.default_timer()\n",
    "# for epoch in range(epoch, num_epochs):\n",
    "#     for i, batch in enumerate(dataloader):\n",
    "#         # Model inputs\n",
    "#         imgs_distorted = Variable(batch[\"A\"].type(Tensor))\n",
    "#         imgs_good_gt = Variable(batch[\"B\"].type(Tensor))\n",
    "#         # Adversarial ground truths\n",
    "#         valid = Variable(Tensor(np.ones((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "#         fake = Variable(Tensor(np.zeros((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "#         ## Train Discriminator\n",
    "#         optimizer_D.zero_grad()\n",
    "#         imgs_fake = generator(imgs_distorted)\n",
    "#         pred_real = discriminator(imgs_good_gt, imgs_distorted)\n",
    "#         loss_real = Adv_cGAN(pred_real, valid)\n",
    "#         pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "#         loss_fake = Adv_cGAN(pred_fake, fake)\n",
    "#         # Total loss: real + fake (standard PatchGAN)\n",
    "#         loss_D = 0.5 * (loss_real + loss_fake) * 10.0 # 10x scaled for stability\n",
    "#         loss_D.backward()\n",
    "#         optimizer_D.step()\n",
    "\n",
    "#         ## Train Generator\n",
    "#         optimizer_G.zero_grad()\n",
    "#         imgs_fake = generator(imgs_distorted)\n",
    "#         pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "#         loss_GAN =  Adv_cGAN(pred_fake, valid) # GAN loss\n",
    "#         loss_1 = L1_G(imgs_fake, imgs_good_gt) # similarity loss\n",
    "#         loss_con = L_vgg(imgs_fake, imgs_good_gt)# content loss\n",
    "#         # Total loss (Section 3.2.1 in the paper)\n",
    "#         loss_G = loss_GAN + lambda_1 * loss_1  + lambda_con * loss_con \n",
    "#         loss_G.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         ## Print log\n",
    "#         if not i%50:\n",
    "#             sys.stdout.write(\"\\r[Epoch %d/%d: batch %d/%d] [DLoss: %.3f, GLoss: %.3f, AdvLoss: %.3f]\"\n",
    "#                               %(\n",
    "#                                 epoch, num_epochs, i, len(dataloader),\n",
    "#                                 loss_D.item(), loss_G.item(), loss_GAN.item(),\n",
    "#                                )\n",
    "#             )\n",
    "#         ## If at sample interval save image\n",
    "#         batches_done = epoch * len(dataloader) + i\n",
    "#         if batches_done % val_interval == 0:\n",
    "#             imgs = next(iter(val_dataloader))\n",
    "#             imgs_val = Variable(imgs[\"val\"].type(Tensor))\n",
    "#             imgs_gen = generator(imgs_val)\n",
    "#             img_sample = torch.cat((imgs_val.data, imgs_gen.data), -2)\n",
    "#             save_image(img_sample, \"samples/FunieGAN/%s/%s.png\" % (dataset_name, batches_done), nrow=5, normalize=True)\n",
    "\n",
    "#     ## Save model checkpoints\n",
    "#     if (epoch % ckpt_interval == 0):\n",
    "#         torch.save(generator.state_dict(), \"checkpoints/FunieGAN/%s/generator_%d.pth\" % (dataset_name, epoch))\n",
    "#         torch.save(discriminator.state_dict(), \"checkpoints/FunieGAN/%s/discriminator_%d.pth\" % (dataset_name, epoch))\n",
    "\n",
    "#     end_time = timeit.default_timer()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa01d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pubgp\\AppData\\Local\\Temp\\ipykernel_66520\\904168020.py:12: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  valid = Variable(Tensor(np.ones((imgs_distorted.size(0), *patch))), requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/60: batch 100/114] [DLoss: 2.556, GLoss: 1.355, AdvLoss: 0.278, SSIMLoss: 0.202]\n",
      "Elapsed time: 96.39 seconds\n",
      "[Epoch 2/60: batch 100/114] [DLoss: 2.484, GLoss: 1.055, AdvLoss: 0.339, SSIMLoss: 0.139]\n",
      "Elapsed time: 213.40 seconds\n",
      "[Epoch 3/60: batch 100/114] [DLoss: 2.594, GLoss: 0.924, AdvLoss: 0.282, SSIMLoss: 0.119]\n",
      "Elapsed time: 323.25 seconds\n",
      "[Epoch 4/60: batch 100/114] [DLoss: 2.451, GLoss: 0.806, AdvLoss: 0.247, SSIMLoss: 0.122]\n",
      "Elapsed time: 432.25 seconds\n",
      "[Epoch 5/60: batch 100/114] [DLoss: 2.533, GLoss: 0.979, AdvLoss: 0.360, SSIMLoss: 0.110]\n",
      "Elapsed time: 529.91 seconds\n",
      "[Epoch 6/60: batch 100/114] [DLoss: 2.275, GLoss: 0.940, AdvLoss: 0.365, SSIMLoss: 0.105]\n",
      "Elapsed time: 630.13 seconds\n",
      "[Epoch 7/60: batch 100/114] [DLoss: 2.422, GLoss: 0.709, AdvLoss: 0.265, SSIMLoss: 0.086]\n",
      "Elapsed time: 719.87 seconds\n",
      "[Epoch 8/60: batch 100/114] [DLoss: 2.558, GLoss: 0.647, AdvLoss: 0.253, SSIMLoss: 0.072]\n",
      "Elapsed time: 793.93 seconds\n",
      "[Epoch 9/60: batch 100/114] [DLoss: 2.570, GLoss: 0.737, AdvLoss: 0.302, SSIMLoss: 0.082]\n",
      "Elapsed time: 871.65 seconds\n",
      "[Epoch 10/60: batch 100/114] [DLoss: 2.438, GLoss: 0.785, AdvLoss: 0.283, SSIMLoss: 0.098]\n",
      "Elapsed time: 964.92 seconds\n",
      "[Epoch 11/60: batch 100/114] [DLoss: 2.438, GLoss: 0.776, AdvLoss: 0.349, SSIMLoss: 0.083]\n",
      "Elapsed time: 1053.91 seconds\n",
      "[Epoch 12/60: batch 100/114] [DLoss: 2.485, GLoss: 0.644, AdvLoss: 0.286, SSIMLoss: 0.073]\n",
      "Elapsed time: 1143.34 seconds\n",
      "[Epoch 13/60: batch 100/114] [DLoss: 2.499, GLoss: 0.583, AdvLoss: 0.255, SSIMLoss: 0.066]\n",
      "Elapsed time: 1230.95 seconds\n",
      "[Epoch 14/60: batch 100/114] [DLoss: 2.230, GLoss: 0.636, AdvLoss: 0.305, SSIMLoss: 0.065]\n",
      "Elapsed time: 1318.07 seconds\n",
      "[Epoch 15/60: batch 100/114] [DLoss: 2.486, GLoss: 0.661, AdvLoss: 0.294, SSIMLoss: 0.072]\n",
      "Elapsed time: 1407.72 seconds\n",
      "[Epoch 16/60: batch 100/114] [DLoss: 2.523, GLoss: 0.690, AdvLoss: 0.341, SSIMLoss: 0.072]\n",
      "Elapsed time: 1498.99 seconds\n",
      "[Epoch 17/60: batch 100/114] [DLoss: 2.370, GLoss: 0.656, AdvLoss: 0.278, SSIMLoss: 0.073]\n",
      "Elapsed time: 1587.24 seconds\n",
      "[Epoch 18/60: batch 100/114] [DLoss: 2.325, GLoss: 0.804, AdvLoss: 0.377, SSIMLoss: 0.078]\n",
      "Elapsed time: 1673.74 seconds\n",
      "[Epoch 19/60: batch 100/114] [DLoss: 2.628, GLoss: 0.623, AdvLoss: 0.295, SSIMLoss: 0.065]\n",
      "Elapsed time: 1764.19 seconds\n",
      "[Epoch 20/60: batch 100/114] [DLoss: 2.444, GLoss: 0.596, AdvLoss: 0.270, SSIMLoss: 0.057]\n",
      "Elapsed time: 1855.39 seconds\n",
      "[Epoch 21/60: batch 100/114] [DLoss: 2.452, GLoss: 0.594, AdvLoss: 0.266, SSIMLoss: 0.060]\n",
      "Elapsed time: 1942.32 seconds\n",
      "[Epoch 22/60: batch 100/114] [DLoss: 3.671, GLoss: 0.395, AdvLoss: 0.050, SSIMLoss: 0.069]\n",
      "Elapsed time: 2031.49 seconds\n",
      "[Epoch 23/60: batch 100/114] [DLoss: 2.472, GLoss: 0.569, AdvLoss: 0.246, SSIMLoss: 0.059]\n",
      "Elapsed time: 2118.99 seconds\n",
      "[Epoch 24/60: batch 100/114] [DLoss: 2.475, GLoss: 0.547, AdvLoss: 0.251, SSIMLoss: 0.061]\n",
      "Elapsed time: 2209.59 seconds\n",
      "[Epoch 25/60: batch 100/114] [DLoss: 1.230, GLoss: 1.202, AdvLoss: 0.757, SSIMLoss: 0.095]\n",
      "Elapsed time: 2302.90 seconds\n",
      "[Epoch 26/60: batch 100/114] [DLoss: 0.783, GLoss: 1.122, AdvLoss: 0.772, SSIMLoss: 0.073]\n",
      "Elapsed time: 2394.91 seconds\n",
      "[Epoch 27/60: batch 100/114] [DLoss: 1.704, GLoss: 1.085, AdvLoss: 0.731, SSIMLoss: 0.069]\n",
      "Elapsed time: 2487.52 seconds\n",
      "[Epoch 28/60: batch 100/114] [DLoss: 0.668, GLoss: 1.798, AdvLoss: 1.427, SSIMLoss: 0.073]\n",
      "Elapsed time: 2580.07 seconds\n",
      "[Epoch 29/60: batch 100/114] [DLoss: 2.361, GLoss: 0.997, AdvLoss: 0.601, SSIMLoss: 0.077]\n",
      "Elapsed time: 2671.33 seconds\n",
      "[Epoch 30/60: batch 100/114] [DLoss: 2.109, GLoss: 1.510, AdvLoss: 1.113, SSIMLoss: 0.078]\n",
      "Elapsed time: 2759.78 seconds\n",
      "[Epoch 31/60: batch 100/114] [DLoss: 0.336, GLoss: 1.187, AdvLoss: 0.795, SSIMLoss: 0.077]\n",
      "Elapsed time: 2846.15 seconds\n",
      "[Epoch 32/60: batch 100/114] [DLoss: 1.486, GLoss: 1.018, AdvLoss: 0.620, SSIMLoss: 0.080]\n",
      "Elapsed time: 2934.25 seconds\n",
      "[Epoch 33/60: batch 100/114] [DLoss: 1.469, GLoss: 0.910, AdvLoss: 0.501, SSIMLoss: 0.077]\n",
      "Elapsed time: 3028.71 seconds\n",
      "[Epoch 34/60: batch 100/114] [DLoss: 1.955, GLoss: 0.944, AdvLoss: 0.586, SSIMLoss: 0.069]\n",
      "Elapsed time: 3115.66 seconds\n",
      "[Epoch 35/60: batch 100/114] [DLoss: 1.222, GLoss: 1.098, AdvLoss: 0.659, SSIMLoss: 0.085]\n",
      "Elapsed time: 3205.36 seconds\n",
      "[Epoch 36/60: batch 100/114] [DLoss: 1.266, GLoss: 1.105, AdvLoss: 0.695, SSIMLoss: 0.076]\n",
      "Elapsed time: 3297.25 seconds\n",
      "[Epoch 37/60: batch 100/114] [DLoss: 2.454, GLoss: 0.630, AdvLoss: 0.278, SSIMLoss: 0.062]\n",
      "Elapsed time: 3399.99 seconds\n",
      "[Epoch 38/60: batch 100/114] [DLoss: 0.592, GLoss: 1.084, AdvLoss: 0.725, SSIMLoss: 0.072]\n",
      "Elapsed time: 3502.57 seconds\n",
      "[Epoch 39/60: batch 100/114] [DLoss: 2.535, GLoss: 1.114, AdvLoss: 0.752, SSIMLoss: 0.072]\n",
      "Elapsed time: 3598.89 seconds\n",
      "[Epoch 40/60: batch 100/114] [DLoss: 1.672, GLoss: 1.180, AdvLoss: 0.772, SSIMLoss: 0.077]\n",
      "Elapsed time: 3694.37 seconds\n",
      "[Epoch 41/60: batch 100/114] [DLoss: 1.699, GLoss: 1.487, AdvLoss: 1.070, SSIMLoss: 0.083]\n",
      "Elapsed time: 3790.52 seconds\n",
      "[Epoch 42/60: batch 100/114] [DLoss: 1.312, GLoss: 1.041, AdvLoss: 0.650, SSIMLoss: 0.086]\n",
      "Elapsed time: 3890.58 seconds\n",
      "[Epoch 43/60: batch 100/114] [DLoss: 1.139, GLoss: 0.998, AdvLoss: 0.595, SSIMLoss: 0.079]\n",
      "Elapsed time: 3988.82 seconds\n",
      "[Epoch 44/60: batch 100/114] [DLoss: 2.412, GLoss: 0.922, AdvLoss: 0.585, SSIMLoss: 0.064]\n",
      "Elapsed time: 4085.33 seconds\n",
      "[Epoch 45/60: batch 100/114] [DLoss: 1.282, GLoss: 1.197, AdvLoss: 0.834, SSIMLoss: 0.068]\n",
      "Elapsed time: 4181.58 seconds\n",
      "[Epoch 46/60: batch 100/114] [DLoss: 1.902, GLoss: 0.857, AdvLoss: 0.427, SSIMLoss: 0.085]\n",
      "Elapsed time: 4278.45 seconds\n",
      "[Epoch 47/60: batch 100/114] [DLoss: 1.280, GLoss: 1.040, AdvLoss: 0.662, SSIMLoss: 0.083]\n",
      "Elapsed time: 4377.55 seconds\n",
      "[Epoch 48/60: batch 100/114] [DLoss: 2.204, GLoss: 1.275, AdvLoss: 0.875, SSIMLoss: 0.076]\n",
      "Elapsed time: 4479.38 seconds\n",
      "[Epoch 49/60: batch 100/114] [DLoss: 1.135, GLoss: 1.113, AdvLoss: 0.686, SSIMLoss: 0.081]\n",
      "Elapsed time: 4582.99 seconds\n",
      "[Epoch 50/60: batch 100/114] [DLoss: 1.111, GLoss: 1.061, AdvLoss: 0.654, SSIMLoss: 0.079]\n",
      "Elapsed time: 4671.88 seconds\n",
      "[Epoch 51/60: batch 100/114] [DLoss: 1.232, GLoss: 1.003, AdvLoss: 0.636, SSIMLoss: 0.070]\n",
      "Elapsed time: 4771.06 seconds\n",
      "[Epoch 52/60: batch 100/114] [DLoss: 1.265, GLoss: 1.059, AdvLoss: 0.600, SSIMLoss: 0.085]\n",
      "Elapsed time: 4874.55 seconds\n",
      "[Epoch 53/60: batch 100/114] [DLoss: 1.163, GLoss: 1.128, AdvLoss: 0.767, SSIMLoss: 0.072]\n",
      "Elapsed time: 4971.50 seconds\n",
      "[Epoch 54/60: batch 100/114] [DLoss: 1.218, GLoss: 0.988, AdvLoss: 0.630, SSIMLoss: 0.067]\n",
      "Elapsed time: 5069.63 seconds\n",
      "[Epoch 55/60: batch 100/114] [DLoss: 0.945, GLoss: 0.989, AdvLoss: 0.642, SSIMLoss: 0.070]\n",
      "Elapsed time: 5169.75 seconds\n",
      "[Epoch 56/60: batch 100/114] [DLoss: 1.379, GLoss: 0.890, AdvLoss: 0.518, SSIMLoss: 0.082]\n",
      "Elapsed time: 5264.90 seconds\n",
      "[Epoch 57/60: batch 100/114] [DLoss: 0.516, GLoss: 0.963, AdvLoss: 0.598, SSIMLoss: 0.073]\n",
      "Elapsed time: 5360.32 seconds\n",
      "[Epoch 58/60: batch 100/114] [DLoss: 2.313, GLoss: 0.674, AdvLoss: 0.304, SSIMLoss: 0.075]\n",
      "Elapsed time: 5469.98 seconds\n",
      "[Epoch 59/60: batch 100/114] [DLoss: 1.242, GLoss: 1.134, AdvLoss: 0.784, SSIMLoss: 0.068]\n",
      "Elapsed time: 5581.06 seconds\n"
     ]
    }
   ],
   "source": [
    "from pytorch_msssim import ssim\n",
    "import timeit\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "for epoch in range(epoch, num_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # === Model inputs ===\n",
    "        imgs_distorted = Variable(batch[\"A\"].type(Tensor))\n",
    "        imgs_good_gt = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "        # === Ground truths for PatchGAN ===\n",
    "        valid = Variable(Tensor(np.ones((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "        fake = Variable(Tensor(np.zeros((imgs_distorted.size(0), *patch))), requires_grad=False)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        optimizer_D.zero_grad()\n",
    "        imgs_fake = generator(imgs_distorted).detach()\n",
    "        pred_real = discriminator(imgs_good_gt, imgs_distorted)\n",
    "        loss_real = Adv_cGAN(pred_real, valid)\n",
    "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "        loss_fake = Adv_cGAN(pred_fake, fake)\n",
    "        loss_D = 0.5 * (loss_real + loss_fake) * 10.0\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        optimizer_G.zero_grad()\n",
    "        imgs_fake = generator(imgs_distorted)\n",
    "        pred_fake = discriminator(imgs_fake, imgs_distorted)\n",
    "        loss_GAN = Adv_cGAN(pred_fake, valid)\n",
    "        loss_1 = L1_G(imgs_fake, imgs_good_gt)\n",
    "        loss_con = L_vgg(imgs_fake, imgs_good_gt)\n",
    "        loss_ssim = 1 - ssim(imgs_fake, imgs_good_gt, data_range=1.0, size_average=True)\n",
    "\n",
    "        # === Total Generator Loss ===\n",
    "        lambda_ssim = 2  # you can tune this\n",
    "        loss_G = loss_GAN + lambda_1 * loss_1 + lambda_con * loss_con + lambda_ssim * loss_ssim\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # === Logging ===\n",
    "        if not i % 50:\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d: batch %d/%d] [DLoss: %.3f, GLoss: %.3f, AdvLoss: %.3f, SSIMLoss: %.3f]\"\n",
    "                % (\n",
    "                    epoch, num_epochs, i, len(dataloader),\n",
    "                    loss_D.item(), loss_G.item(), loss_GAN.item(), loss_ssim.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # === Validation ===\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % val_interval == 0:\n",
    "            imgs = next(iter(val_dataloader))\n",
    "            imgs_val = Variable(imgs[\"val\"].type(Tensor))\n",
    "            imgs_gen = generator(imgs_val)\n",
    "            img_sample = torch.cat((imgs_val.data, imgs_gen.data), -2)\n",
    "            save_image(img_sample, f\"samples/FunieGAN/{dataset_name}/{batches_done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # === Checkpointing ===\n",
    "    if (epoch % ckpt_interval == 0):\n",
    "        torch.save(generator.state_dict(), f\"checkpoints/FunieGAN/{dataset_name}/generator_{epoch}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"checkpoints/FunieGAN/{dataset_name}/discriminator_{epoch}.pth\")\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fafd6ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"checkpoints/FunieGAN/paired/generator_60.pth\")\n",
    "torch.save(discriminator.state_dict(), \"checkpoints/FunieGAN/paired/discriminator_60.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix2pix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
